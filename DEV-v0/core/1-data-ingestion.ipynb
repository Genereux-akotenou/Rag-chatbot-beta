{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data ingestion using langchain and chomeDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install chromadb\n",
    "#!pip install transformers\n",
    "#!pip install -qU \"langchain-chroma>=0.1.2\"\n",
    "#!pip install scholarly\n",
    "#!pip install pdfkit\n",
    "#!pip install    langchain pymupdf\n",
    "#!pip install -U langchain-community\n",
    "#!pip install --upgrade langchain chromadb\n",
    "#!pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, pdfkit, torch, chromadb\n",
    "\n",
    "from chromadb import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_chroma import Chroma\n",
    "from scholarly import scholarly\n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Innital embedding and vectorDB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Init chomeDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained transformer model for embeddings\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "class EmbeddingFunction():\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        return embeddings.numpy()\n",
    "    \n",
    "    def embed_query(self, query):\n",
    "        inputs = self.tokenizer([query], padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            embedding = self.model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        return embedding.numpy().squeeze()\n",
    "\n",
    "embedding_function = EmbeddingFunction(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Integrate chomaDB with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"rag_4_researcher_collection\",\n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory=\"../database/vector-db/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fetch data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, title, output_dir=\"../database/document\"):\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        safe_title = \"\".join(c for c in title if c.isalnum() or c in (\" \", \"_\")).rstrip()\n",
    "        file_path = os.path.join(output_dir, f\"{safe_title}.pdf\")\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200 and 'application/pdf' in response.headers.get('Content-Type', ''):\n",
    "            with open(file_path, 'wb') as pdf_file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    pdf_file.write(chunk)\n",
    "            print(f\"Downloaded: {file_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to download PDF from {url}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def convert_webpage_to_pdf(url, output_dir=\"../database/documents\", filename=\"converted_page\"):\n",
    "    filename += str(uuid4()) + \".pdf\"\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Convert webpage to PDF\n",
    "        pdfkit.from_url(url, file_path)\n",
    "        print(f\"PDF successfully saved at: {file_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting URL to PDF: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "def get_scholar_urls(query, max_results=10):\n",
    "    try:\n",
    "        # Search for the query\n",
    "        search_results = scholarly.search_pubs(query)\n",
    "        urls = []\n",
    "        count = 0\n",
    "\n",
    "        for result in search_results:\n",
    "            if count >= max_results:\n",
    "                break\n",
    "            count += 1\n",
    "\n",
    "            # Get the title and URL\n",
    "            title = result.get('bib', {}).get('title', 'No title available')\n",
    "            url = result.get('eprint_url') or result.get('pub_url') or 'No URL available'\n",
    "\n",
    "            if url != 'No URL available':\n",
    "                urls.append({\"title\": title, \"url\": url})\n",
    "\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping ... : 1it [00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../database/document/GenoTEX A Benchmark for Evaluating LLMBased Exploration of Gene Expression Data in Alignment with Bioinformaticians.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping ... : 2it [00:02,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../database/document/Phenomics Assistant An Interface for LLMbased Biomedical Knowledge Graph Exploration.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping ... : 3it [00:03,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../database/document/GPGPT Large Language Model for GenePhenotype Mapping.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping ... : 4it [00:07,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../database/document/BioinfoBench A Simple Benchmark Framework for LLM Bioinformatics Skills Evaluation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping ... : 5it [00:11,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../database/document/Leveraging genomic large language models to enhance causal genotypebrainclinical pathways in Alzheimers disease.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping ... : 7it [00:16,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../database/document/Genetic Transformer An Innovative Large Language Model Driven Approach for Rapid and Accurate Identification of Causative Variants in Rare Genetic Diseases.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping ... : 8it [00:17,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../database/document/An llmbased knowledge synthesis and scientific reasoning framework for biomedical discovery.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrapping ... : 10it [00:20,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ../database/document/Geneverse A collection of Opensource Multimodal Large Language Models for Genomic and Proteomic Research.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_term = \"LLM in genomics\"\n",
    "max_results = 10\n",
    "\n",
    "# Get search results\n",
    "results = get_scholar_urls(search_term, max_results=max_results)\n",
    "\n",
    "# Process each result\n",
    "for idx, article in tqdm(enumerate(results), desc=\"Scrapping ... \"):\n",
    "    title = article[\"title\"]\n",
    "    url = article[\"url\"]\n",
    "    #print(f\"{idx+1}. {title}\\nURL: {url}\\n\")\n",
    "\n",
    "    # Check if the URL points to a PDF\n",
    "    if \"/pdf/\" in url or url.endswith(\".pdf\"):\n",
    "        download_pdf(url, title)\n",
    "    # elif \"sciencedirect.com\" in url:\n",
    "    #     print(f\"Skipping ScienceDirect PDF download for now: {url}\")\n",
    "    #else:\n",
    "    #    convert_webpage_to_pdf(url, output_dir=\"../database/document\", filename=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extract Text from pdf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: ../database/document/2407.11435v1.pdf\n",
      "Processed 25 documents.\n"
     ]
    }
   ],
   "source": [
    "directory_path = Path(\"../database/document\")\n",
    "pdf_files = directory_path.glob(\"*.pdf\")\n",
    "documents = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(file_path=str(file_path))\n",
    "        loaded_docs = loader.load()\n",
    "        documents.extend(loaded_docs)\n",
    "        print(f\"Processed file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "print(f\"Processed {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunking we will see latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'metadata', 'page_content', 'type'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].model_dump().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../database/document/2407.11435v1.pdf',\n",
       " 'file_path': '../database/document/2407.11435v1.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 25,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': '',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'producer': 'pdfTeX-1.40.25',\n",
       " 'creationDate': 'D:20240717002846Z',\n",
       " 'modDate': 'D:20240717002846Z',\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87c71949-68f2-4b70-97aa-652669ee92be',\n",
       " '0495e4c0-6cad-408e-84bc-63fbf2b83be5',\n",
       " '5e3c8eaf-3766-418f-846f-3e7554ffb8dc',\n",
       " 'd747312a-3240-4041-a088-a813f416d8ec',\n",
       " '5c96395c-7c5c-4734-b826-4273c7e68aad',\n",
       " 'fe3e580c-542e-4d6c-aa29-932c4f9174e8',\n",
       " 'a1a04faa-4bbb-40d3-b91a-5d832a61d5ec',\n",
       " 'cbd1d0d7-5ef3-42ec-8f21-8a57c98bbe4b',\n",
       " 'b2390c6a-5823-4caa-a448-8f95786c6dce',\n",
       " '5599221b-d0d8-4bf0-96b5-4cfd95465bc2',\n",
       " 'f7ec33bb-3cba-4864-ae65-ab00ef122f72',\n",
       " '129f9678-d321-4130-8646-cbece1fef10c',\n",
       " '7bd4957f-f6a0-4415-8f4e-10d33da7c993',\n",
       " 'b9b05df1-6057-405f-9570-89a56e29aedc',\n",
       " '0e3c868b-3072-4a0b-9f09-ec477f27552b',\n",
       " 'c3db0347-7471-4dce-a4fe-dcb5e84c2530',\n",
       " '333a4ce5-e9dc-4985-93e8-4131b50fe044',\n",
       " 'ec73c1de-8125-47f0-a0bd-fc7b6b3f33c3',\n",
       " '47646f1f-46e6-429f-b51d-5a86a2fab1f4',\n",
       " '21a1cdba-e9f7-41ae-8594-f8dd060a9fdb',\n",
       " 'a5cc8a19-b1dc-4d25-a2c1-814a7562c262',\n",
       " '8f25fcdd-bb26-400b-aa54-9293752d3e5c',\n",
       " '21178bfa-b952-493a-b79c-26f35a7a04d9',\n",
       " '739a55c8-eab0-4699-803e-190b0d50f74b',\n",
       " '91659de5-a28c-4375-9e56-9a66122c3f09']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [{'author': '', 'creationDate': 'D:20240717002846Z', 'creator': 'LaTeX with hyperref', 'file_path': '../database/document/2407.11435v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240717002846Z', 'page': 4, 'producer': 'pdfTeX-1.40.25', 'source': '../database/document/2407.11435v1.pdf', 'subject': '', 'title': '', 'total_pages': 25, 'trapped': ''}]\n",
      "* [{'author': '', 'creationDate': 'D:20240717002846Z', 'creator': 'LaTeX with hyperref', 'file_path': '../database/document/2407.11435v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240717002846Z', 'page': 10, 'producer': 'pdfTeX-1.40.25', 'source': '../database/document/2407.11435v1.pdf', 'subject': '', 'title': '', 'total_pages': 25, 'trapped': ''}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"LLM for genes prediction\",\n",
    "    k=2,\n",
    "    #filter={\"source\": \"tweet\"},\n",
    ")\n",
    "for res in results:\n",
    "    #print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "    print(f\"* [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40911700-1177-4982-932f-ba4e724f04a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "1\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "2\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "3\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n",
      "4\n",
      "j= 0\n",
      "j= 1\n",
      "j= 2\n",
      "j= 3\n",
      "j= 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i)\n",
    "    for j in range(5):\n",
    "        print(\"j=\", j)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340c290-02e7-4b3d-814b-07cef7f5f658",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d54f2b;padding: 1em; color: white;\">\n",
    "<b>Part III</b>: Build Retrieval and Generation Pipeline\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be96d7c-4271-434b-9de9-2202fe581737",
   "metadata": {},
   "source": [
    "- Load embeding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78777a06-1bfc-4f30-947e-2332ff49cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"manu/bge-m3-custom-fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352b2ba-c333-468a-b0e7-1e8b21699001",
   "metadata": {},
   "source": [
    "-  Generate a Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205dc88a-4c5e-4bb8-8cd1-161a2541fbf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#query_str = \"Que dit le règlement financier de l'UM6P ?\"\n",
    "query_str = \"Comment contacter le carrier center ?\"\n",
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298f473-1280-43f3-be09-36fbc068db76",
   "metadata": {},
   "source": [
    "- Query the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ee2bdb-e1c6-49c4-88f9-0d863d3ed72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af85285-5143-4733-87d5-9d2fe07e3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "# DB Parameters\n",
    "db_name = \"rag_vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"rag_password\"\n",
    "port = \"5433\"\n",
    "user = \"rag_user\"\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"rag_paper_fr\",\n",
    "    embed_dim=1024,\n",
    ")\n",
    "\n",
    "query_result = vector_store.query(vector_store_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99004fcf-7685-4294-ab90-7f5a1888d8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0d538711-cf99-46b1-af72-1aff9f98474b',\n",
       " 'c4e5c5b5-9134-4c25-a84f-76c89ab4aff9']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741d8b06-c386-490f-8ee3-458713f0ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.62223181459782, 0.5314870793513788]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result.similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088bf74b-525f-49b2-90bc-12ae81f12551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='0d538711-cf99-46b1-af72-1aff9f98474b', embedding=None, metadata={'total_pages': 2, 'file_path': 'documents/UM6P-Phone_contact.pdf', 'source': '1'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='62\\nCONTACTS\\xa0:\\nAdresse email\\nTéléphone\\nCareer Center\\nComplexe sportif \\nFacilities\\nHealth Center\\nHelpdesk informatique\\nLanguage Lab\\nLearning Center / \\nBibliothèque\\nMahir center\\nBenguerir : Career.center@um6p.ma \\nRabat : SCALE\\nPole.sport@um6p.ma pour le \\ncampus de Benguerir\\nPole.sportcr@um6p.ma pour le \\ncampus de Rabat\\nHébergement : housingrequest@um6p.ma\\nRestauration : cateringrequest@um6p.ma\\nConsultation à distance\\nAstreinte Health Center 7j/7\\net 24H/24\\nhealth.center@um6p.ma \\nBenguérir : helpdesk@um6p.ma \\nAstreinte 7j/7 et 24H/24\\nIT Support RABAT:\\nit-support-rabat@um6p.ma\\nlanguagelab@um6p.ma \\nBenguérir : lc@um6p.ma\\nPortail : https://learningcenter.um6p.ma\\nBureaux des aides documentalistes :\\nBureau 1\\nBureau 2\\nRabat : bibliotheque.fgses@um6p.ma\\nhttps://biblio.fgses-um6p.ma/\\nmahircenter@um6p.ma \\n05 25 07 27 00\\n05 25 07 27 10\\n06 16 14 01 93\\n06 66 96 80 08\\n05 25 07 32 04\\n06 78 82 69 33 \\n05 30 43 15 15\\n05 25 07 28 97 \\n05 25 07 32 00', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='c4e5c5b5-9134-4c25-a84f-76c89ab4aff9', embedding=None, metadata={'total_pages': 2, 'file_path': 'documents/UM6P-Phone_contact.pdf', 'source': '2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='63\\nRegistrariat\\nSOLE (Student \\nOrganizations, Leadership \\nand Engagement)\\nSAC (Student Activities \\nCenter)\\nStartgate\\n1337 - école de \\ncodage\\nBenguerir : anas.benyoussef@um6p.ma\\nRabat : bennaceur.baahmad@um6p.ma\\nregistrariat@um6p.ma \\nsole@um6p.ma\\nsac@um6p.ma\\nhello@startgate.ma\\nhttps://startgate.ma/ \\nhind@1337.ma; Yassir@1337.ma', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9007a6-059b-4d25-bc6d-0d7bf9075389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'documents/UM6P-Phone_contact.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result.nodes[0].metadata['file_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f83b8-b098-463d-acf1-47228179a162",
   "metadata": {},
   "source": [
    "- **Augmented generation**\n",
    "\n",
    "Now that we have a context that can contain the best response for the query, we need to use a LLM to make prompt so it generate response using this context. Here we use Llama2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea59c17-4731-497e-aa4e-71e7e00f5218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/genereux/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.34 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   170.20 MiB, ( 3220.84 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "llm_load_tensors:      Metal buffer size =   170.20 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2973.75 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    76.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 3050.00 MiB, K (f16): 1525.00 MiB, V (f16): 1525.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 627\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    context_window=3900, #up to 4096\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8db04-cc98-43c6-a781-2e183f0cdf55",
   "metadata": {},
   "source": [
    "- Parse Result into a Set of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b7e9fcb-64e2-4d08-a031-5d497e002b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9719f-fec0-41c4-bc89-a5b22ba22ffb",
   "metadata": {},
   "source": [
    "- Put into a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175ee18f-c396-4b80-9639-b01bbc76c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5d4aba-04bd-4ee2-8fe7-db7c45d64d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df56b79-c88d-467b-b109-35c90279dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a48f15-d5a4-4eaf-95c7-c4c3130583e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19076.97 ms\n",
      "llama_print_timings:      sample time =       3.54 ms /    79 runs   (    0.04 ms per token, 22316.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   34495.13 ms /   749 tokens (   46.05 ms per token,    21.71 tokens per second)\n",
      "llama_print_timings:        eval time = 1083920.35 ms /    78 runs   (13896.41 ms per token,     0.07 tokens per second)\n",
      "llama_print_timings:       total time = 1118646.41 ms /   827 tokens\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Comment contacter le carrier center ?\"\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "484e03c6-220f-4d92-827b-998ba779f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le carrier center est accessible par téléphone au 05 25 07 27 00 et par email à career.center@um6p.ma. Vous pouvez également contacter le helpdesk informatique à l'adresse helpdesk@um6p.ma pour obtenir des informations supplémentaires.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0175cf7-a360-4575-9220-47d2991e8f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726bfb8-b231-4ed5-aba3-18e0d15f02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
